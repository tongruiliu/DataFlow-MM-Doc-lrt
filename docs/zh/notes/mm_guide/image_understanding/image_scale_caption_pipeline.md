---
title: ScaleCap é«˜å¯†åº¦æè¿°ç”Ÿæˆæµæ°´çº¿
createTime: 2026/01/11 22:08:57
icon: mdi:image-text
permalink: /zh/mm_guide/image_scale_caption_pipeline/
---

## 1. æ¦‚è¿°

**ScaleCap é«˜å¯†åº¦æè¿°ç”Ÿæˆæµæ°´çº¿ (Image Scale Caption Pipeline)** æ˜¯ä¸€ç§åŸºäº**â€œç”Ÿæˆ-éªŒè¯-æ‰©å±•-èåˆâ€**èŒƒå¼çš„å…ˆè¿›å›¾åƒæè¿°ç”Ÿæˆæ–¹æ¡ˆã€‚è¯¥æµæ°´çº¿æ—¨åœ¨ç”Ÿæˆ**ä¿¡æ¯å¯†åº¦æé«˜**ä¸”**å¹»è§‰ç‡æä½**çš„å›¾åƒæè¿°ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦æ·±åº¦ç†è§£å›¾åƒç»†èŠ‚çš„åœºæ™¯ã€‚

è¯¥æ–¹æ³•çš„ç†è®ºåŸºç¡€æºè‡ªè®ºæ–‡ *ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing*ã€‚å®ƒé€šè¿‡å¤šè½®å¯¹è¯å’Œè§†è§‰è‡ªæ£€ï¼ˆVisual Groundingï¼‰ï¼Œé€æ­¥æŒ–æ˜å›¾åƒä¸­çš„å¯¹è±¡ä¸ä½ç½®ç»†èŠ‚ï¼Œå¹¶è¿‡æ»¤æ‰æ¨¡å‹äº§ç”Ÿçš„å¹»è§‰ã€‚

æˆ‘ä»¬æ”¯æŒä»¥ä¸‹åº”ç”¨åœºæ™¯ï¼š

* **é«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®é›†æ„å»º**ï¼šç”Ÿæˆæ¯”æ™®é€š Caption æ›´è¯¦å°½ã€å‡†ç¡®çš„è®­ç»ƒæ•°æ®ã€‚
* **ç»†ç²’åº¦å›¾åƒæ£€ç´¢**ï¼šæä¾›åŒ…å«ä¸°å¯Œç»†èŠ‚çš„ç´¢å¼•æ–‡æœ¬ã€‚
* **ç›²äººè¾…åŠ©/å›¾åƒæ— éšœç¢**ï¼šç”Ÿæˆâ€œæ‰€è§å³æ‰€å¾—â€çš„è¯¦ç»†è§£è¯´ã€‚

æµæ°´çº¿çš„ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š

1. **åˆç¨¿ç”Ÿæˆ**ï¼šVLM ç”ŸæˆåŸºç¡€æè¿°ã€‚
2. **è§†è§‰è‡ªæ£€ (Debiasing)**ï¼šå°†æè¿°æ‹†åˆ†ä¸ºå¥å­ï¼Œé€å¥éªŒè¯å…¶æ˜¯å¦è¢«å›¾åƒè¯æ®æ”¯æŒï¼ˆVisual Groundingï¼‰ã€‚
3. **ç»†èŠ‚è¿½é—®**ï¼šé’ˆå¯¹é€šè¿‡éªŒè¯çš„â€œé»„é‡‘å¥å­â€ï¼Œç”Ÿæˆå…³äºå¯¹è±¡å±æ€§å’Œä½ç½®çš„è¿½é—®ã€‚
4. **å›ç­”ä¸å†éªŒè¯**ï¼šVLM å›ç­”è¿½é—®ï¼Œå¹¶å†æ¬¡è¿›è¡Œè§†è§‰è‡ªæ£€ä»¥è¿‡æ»¤é”™è¯¯ç»†èŠ‚ã€‚
5. **æœ€ç»ˆèåˆ**ï¼šå°†æ‰€æœ‰ç»è¿‡éªŒè¯çš„ä¿¡æ¯èåˆæˆä¸€æ®µè¿è´¯çš„é•¿æè¿°ã€‚

---

## 2. å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ–°çš„ DataFlow å·¥ä½œæ–‡ä»¶å¤¹

```bash
mkdir run_dataflow
cd run_dataflow

```

### ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ– DataFlow-MM

```bash
dataflowmm init

```

è¿™æ—¶ä½ ä¼šçœ‹åˆ°ï¼š

```bash
gpu_pipelines/image_scale_caption_pipeline.py

```

### ç¬¬ä¸‰æ­¥ï¼šä¸‹è½½ç¤ºä¾‹æ•°æ®

```bash
huggingface-cli download --repo-type dataset OpenDCAI/dataflow-demo-image --local-dir ./example_data

```

### ç¬¬å››æ­¥ï¼šé…ç½®å‚æ•°

```python
if __name__ == "__main__":
    pipe = ImageScaleCaptionPipeline(
        model_path="Qwen/Qwen2.5-VL-3B-Instruct",
        hf_cache_dir="~/.cache/huggingface",
        download_dir="../ckpt/models/Qwen2.5-VL-3B-Instruct",
        device="cuda",
        first_entry_file="../example_data/capsbench_images/image_scale_caption_demo.jsonl",
        cache_path="../cache/image_scale_caption",
        file_name_prefix="scalecap",
        input_image_key="image",
        output_key="final_caption",
        vllm_tensor_parallel_size=1,
        vllm_max_tokens=1024
    )
    pipe.forward()

```

> **âš ï¸ æ¨¡å‹è·¯å¾„é…ç½®çš„é‡è¦æç¤ºï¼ˆä»¥ `Qwen2.5-VL-3B-Instruct` ä¸ºä¾‹ï¼‰ï¼š**
> * **å¦‚æœæ‚¨å·²ç»ä¸‹è½½å¥½äº†æ¨¡å‹æ–‡ä»¶**ï¼šè¯·å°† `model_path` ä¿®æ”¹ä¸ºæ‚¨çš„æœ¬åœ°æ¨¡å‹è·¯å¾„ã€‚**åŠ¡å¿…ä¿è¯**æ¨¡å‹å­˜æ”¾çš„æœ€ç»ˆæ–‡ä»¶å¤¹åç§°ç²¾ç¡®ä¸º `Qwen2.5-VL-3B-Instruct`ï¼Œå¦åˆ™åº•å±‚è§£ææ—¶å°†æ— æ³•æ­£ç¡®åŒ¹é…å’Œè¯†åˆ«è¯¥æ¨¡å‹ã€‚
> * **å¦‚æœæ‚¨è¿˜æœªä¸‹è½½æ¨¡å‹ï¼ˆéœ€è¦è‡ªåŠ¨ä¸‹è½½ï¼‰**ï¼šè¯·ä¸€å®šè¦æŒ‡å®š `download_dir` å‚æ•°ï¼Œå¹¶ä¸”è¯¥ç›®å½•è·¯å¾„**å¿…é¡»ä»¥** `Qwen2.5-VL-3B-Instruct` **ç»“å°¾**ï¼ˆæ­£å¦‚é»˜è®¤å‚æ•°æ‰€ç¤ºï¼‰ï¼Œå¦åˆ™ä¸‹è½½å®ŒæˆååŒæ ·ä¼šå¯¼è‡´æ¡†æ¶æ— æ³•è¯†åˆ«æ¨¡å‹ã€‚
> 
> 

### ç¬¬äº”æ­¥ï¼šä¸€é”®è¿è¡Œ

```bash
cd gpu_pipelines
python image_scale_caption_pipeline.py

```

> **ğŸ› ï¸ å¸¸è§é—®é¢˜æ’æŸ¥ (Troubleshooting)**
> **é—®é¢˜ 1ï¼š** å¦‚æœé‡åˆ°ç±»ä¼¼å¦‚ä¸‹çš„åŠ¨æ€é“¾æ¥åº“å†²çªæŠ¥é”™ï¼š
> `ImportError: .../miniconda3/envs/Dataflow-MM/lib/python3.12/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12`
> **è§£å†³æ–¹æ³•ï¼š** è¿™é€šå¸¸æ˜¯ç¯å¢ƒå˜é‡å¹²æ‰°å¯¼è‡´çš„ã€‚è¯·åœ¨è¿è¡Œå‘½ä»¤å‰æ¸…ç©º `LD_LIBRARY_PATH`ï¼š
> ```bash
> LD_LIBRARY_PATH="" python image_scale_caption_pipeline.py
> 
> ```
> 
> 
> **é—®é¢˜ 2ï¼š** å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ **Qwen ç³»åˆ—æ¨¡å‹**ï¼Œå¹¶ä¸”é‡åˆ°ä»¥ä¸‹æŠ¥é”™ï¼š
> `KeyError: "Missing required keys in rope_scaling for 'rope_type'='None': {'rope_type'}"`
> **è§£å†³æ–¹æ³•ï¼š** æ‰“å¼€æ¨¡å‹æ–‡ä»¶å¤¹ä¸‹çš„ `config.json` æ–‡ä»¶ï¼Œæ‰¾åˆ° `rope_scaling` é…ç½®å—ï¼Œå°† `"type"` å­—æ®µä¿®æ”¹ä¸º `"rope_type"` å³å¯ã€‚
> **ä¿®æ”¹å‰ï¼š**
> ```json
> "rope_scaling": {
>   "type": "mrope",
>   "mrope_section": [
>     16,
>     24,
>     24
>   ]
> }
> 
> ```
> 
> 
> **ä¿®æ”¹åï¼š**
> ```json
> "rope_scaling": {
>   "rope_type": "mrope",
>   "mrope_section": [
>     16,
>     24,
>     24
>   ]
> }
> 
> ```
> 
> 

---

## 3. æ•°æ®æµä¸æµæ°´çº¿é€»è¾‘

### 1. **è¾“å…¥æ•°æ®**

è¾“å…¥æ•°æ®éå¸¸ç®€å•ï¼Œä»…éœ€å›¾åƒè·¯å¾„ï¼š

* **image**ï¼šå›¾åƒæ–‡ä»¶è·¯å¾„ã€‚

**è¾“å…¥æ•°æ®ç¤ºä¾‹**ï¼š

```json
{
    "image": "../example_data/capsbench_images/0.png"
}

```

### 2. **æ ¸å¿ƒç®—å­é€»è¾‘**

è¯¥æµæ°´çº¿æ˜¯å¤šä¸ªåŸå­ç®—å­çš„å¤æ‚ç¼–æ’ï¼š

#### A. **åˆç¨¿ç”Ÿæˆ (PromptedVQAGenerator)**

* **åŠŸèƒ½**ï¼šä½¿ç”¨åŸºç¡€ Prompt ç”Ÿæˆå›¾åƒçš„åˆæ­¥æè¿° (`init_caption`)ã€‚

#### B. **è§†è§‰è‡ªæ£€ (VisualGroundingRefiner)**

* **åŠŸèƒ½**ï¼šè¿™æ˜¯ ScaleCap çš„æ ¸å¿ƒé˜²å¹»è§‰æœºåˆ¶ã€‚
* **é€»è¾‘**ï¼š
1. ä½¿ç”¨ `split_sentences` å°†åˆç¨¿æ‹†åˆ†ä¸ºå•å¥ã€‚
2. è°ƒç”¨ VLM è¯¢é—®ï¼šâ€œGiven the image, is the description '{text}' directly supported by visual evidence?â€ã€‚
3. ä»…ä¿ç•™å›ç­”ä¸º "Yes" çš„å¥å­ï¼Œå½¢æˆ **"Golden Sentences"**ã€‚



#### C. **é—®é¢˜ç”Ÿæˆä¸è§£æ (PromptTemplatedQAGenerator)**

* **åŠŸèƒ½**ï¼šåŸºäº Golden Sentencesï¼Œåˆ©ç”¨ LLM èƒ½åŠ›ç”Ÿæˆé’ˆå¯¹æ€§çš„è¿½é—®ã€‚
* **é€»è¾‘**ï¼šæ¨¡å‹ç”Ÿæˆå¦‚ "Describe more details about the [Object]" çš„æ–‡æœ¬ï¼Œå¹¶é€šè¿‡ `parse_questions_logic` è‡ªåŠ¨æ‰©å±•ä¸º**å¯¹è±¡ç»†èŠ‚**å’Œ**ä½ç½®å…³ç³»**ä¸¤ç±»é—®é¢˜ã€‚

#### D. **æ‰¹é‡å›ç­”ä¸äºŒæ¬¡è¿‡æ»¤ (BatchVQAGenerator & Refiner)**

* **åŠŸèƒ½**ï¼šæŒ–æ˜å›¾åƒæ·±å±‚ä¿¡æ¯ã€‚
* **é€»è¾‘**ï¼š
1. ä½¿ç”¨ `BatchVQAGenerator` ä¸€æ¬¡æ€§è®© VLM å›ç­”ä¸Šè¿°ç”Ÿæˆçš„æ‰€æœ‰é—®é¢˜ã€‚
2. å†æ¬¡ä½¿ç”¨ `VisualGroundingRefiner` æ£€æŸ¥è¿™äº›æ–°ç”Ÿæˆçš„ç»†èŠ‚æ˜¯å¦å‡†ç¡®ã€‚
3. ä¿ç•™å¯é çš„ç»†èŠ‚ä¿¡æ¯ (`final_details`)ã€‚



#### E. **æœ€ç»ˆèåˆ (PromptTemplatedQAGenerator)**

* **åŠŸèƒ½**ï¼šå°†â€œé»„é‡‘å¥å­â€å’Œâ€œéªŒè¯åçš„ç»†èŠ‚â€é‡å†™ä¸ºä¸€æ®µæµç•…çš„æ–‡æœ¬ã€‚
* **è¾“å‡º**ï¼š`final_caption`ã€‚

### 3. **è¾“å‡ºæ•°æ®**

è¾“å‡ºæ•°æ®è®°å½•äº†æµæ°´çº¿çš„å…¨è¿‡ç¨‹ï¼Œæ–¹ä¾¿è°ƒè¯•å’Œåˆ†æï¼š

* **init_caption**ï¼šåŸå§‹ç”Ÿæˆçš„åˆç¨¿ã€‚
* **golden_sentences**ï¼šé€šè¿‡ç¬¬ä¸€æ¬¡è‡ªæ£€çš„å¥å­åˆ—è¡¨ã€‚
* **q_list**ï¼šç”Ÿæˆçš„è¿½é—®åˆ—è¡¨ã€‚
* **final_details**ï¼šé€šè¿‡ç¬¬äºŒæ¬¡è‡ªæ£€çš„ç»†èŠ‚å›ç­”ã€‚
* **final_caption**ï¼šæœ€ç»ˆçš„é«˜å¯†åº¦æè¿°ã€‚

**è¾“å‡ºæ•°æ®ç¤ºä¾‹**ï¼š

```json
{
    "image": "../example_data/capsbench_images/0.png",
    "init_caption": "A dog sitting on a bench.",
    "golden_sentences": ["A dog is sitting on a wooden bench."],
    "q_list": ["Describe more details about the dog.", "Describe more details about the position of the bench."],
    "final_details": ["The dog is a Golden Retriever with a red collar.", "The bench is located in a park."],
    "final_caption": "A Golden Retriever with a red collar is sitting on a wooden bench located in a park."
}

```

---

## 4. æµæ°´çº¿ç¤ºä¾‹

ä»¥ä¸‹æ˜¯å®Œæ•´çš„ `ImageScaleCaptionPipeline` ä»£ç å®ç° (GPU ç‰ˆæœ¬)ã€‚

```python
import re
import argparse
from typing import Callable, Any, List

from dataflow.utils.storage import FileStorage

from dataflow.serving.local_model_vlm_serving import LocalModelVLMServing_vllm

from dataflow.prompts.prompt_template import NamedPlaceholderPromptTemplate
from dataflow.prompts.image import ImageScaleCaptionPrompt

from dataflow.operators.core_vision import PromptedVQAGenerator, BatchVQAGenerator, VisualGroundingRefiner
from dataflow.operators.core_text import PromptTemplatedQAGenerator, FunctionalRefiner


def split_sentences(text: str) -> List[str]:
    """å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­åˆ—è¡¨"""
    if not text or not isinstance(text, str):
        return []
    # ä½¿ç”¨æ­£åˆ™æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰² (. ! ? ã€‚ ï¼ ï¼Ÿ)
    _SENT_SPLIT = re.compile(r"(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+")
    parts = [p.strip() for p in _SENT_SPLIT.split(text) if p.strip()]
    return parts or ([text.strip()] if text.strip() else [])

def join_list(data: Any, separator: str = "\n") -> str:
    """å°†åˆ—è¡¨è¿æ¥ä¸ºå­—ç¬¦ä¸²"""
    if isinstance(data, list):
        # è¿‡æ»¤æ‰éå­—ç¬¦ä¸²å…ƒç´ æˆ–ç©ºå­—ç¬¦ä¸²
        valid_items = [str(x) for x in data if x]
        return separator.join(valid_items)
    return str(data) if data is not None else ""

def parse_questions_logic(text: str, max_q: int = 20) -> List[str]:
    """
    è§£æ LLM ç”Ÿæˆçš„ "Describe more details about..." æ–‡æœ¬ï¼Œ
    å¹¶è‡ªåŠ¨æ‰©å±• position é—®é¢˜ã€‚
    """
    if not text or not isinstance(text, str):
        return []

    lines = [t.strip() for t in text.split("\n") if t.strip()]
    obj_qs = []
    
    for line in lines:
        # æå–åŒ…å« "Describe more details about" çš„è¡Œ
        if "Describe more details about" in line:
            # å»é™¤å¯èƒ½çš„åºå· (å¦‚ "1. Describe...")
            try:
                start_idx = line.find("Describe")
                clean = line[start_idx:]
                # å»é™¤å¥æœ«å¤šä½™å†…å®¹ï¼Œä¿ç•™åˆ°ç¬¬ä¸€ä¸ªå¥å·
                if "." in clean:
                    clean = clean.split(".")[0] + "."
                obj_qs.append(clean)
            except Exception:
                continue
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    unique_obj_qs = []
    for q in obj_qs:
        if q not in seen:
            unique_obj_qs.append(q)
            seen.add(q)
    
    # æˆªæ–­
    unique_obj_qs = unique_obj_qs[:max_q]
    
    # æ‰©å±• Position é—®é¢˜
    pos_qs = [
        q.replace("Describe more details about", "Describe more details about the position of")
        for q in unique_obj_qs
    ]
    
    # è¿”å›åˆå¹¶åçš„åˆ—è¡¨ (å¯¹è±¡é—®é¢˜ + ä½ç½®é—®é¢˜)
    return unique_obj_qs + pos_qs


class ImageScaleCaptionPipeline:
    def __init__(
        self,
        model_path: str,
        *,
        hf_cache_dir: str | None = None,
        download_dir: str = "./ckpt/models",
        device: str = "cuda",
        # Storage params
        first_entry_file: str = "images.jsonl",
        cache_path: str = "./cache_scalecap",
        file_name_prefix: str = "scalecap",
        cache_type: str = "jsonl",
        # Keys
        input_image_key: str = "image",
        output_key: str = "final_caption",
        # VLLM Config
        vllm_tensor_parallel_size: int = 1,
        vllm_temperature: float = 0.7,
        vllm_top_p: float = 0.9,
        vllm_max_tokens: int = 512,
    ):
        # 1. Storage
        self.storage = FileStorage(
            first_entry_file_name=first_entry_file,
            cache_path=cache_path,
            file_name_prefix=file_name_prefix,
            cache_type=cache_type,
        )

        # 2. Serving
        self.serving = LocalModelVLMServing_vllm(
            hf_model_name_or_path=model_path,
            hf_cache_dir=hf_cache_dir,
            hf_local_dir=download_dir,
            vllm_tensor_parallel_size=vllm_tensor_parallel_size,
            vllm_temperature=vllm_temperature,
            vllm_top_p=vllm_top_p,
            vllm_max_tokens=vllm_max_tokens,
        )

        # 3. Prompts
        self.prompts_db = ImageScaleCaptionPrompt().build_prompt()

        # 4. Keys
        self.input_image_key = input_image_key
        self.output_key = output_key

        # ================== Operator Initialization ==================

        # --- Step A: Generate Init Caption ---
        # æ„é€ å›ºå®š Prompt åˆ—
        self.refine_const_prompt = FunctionalRefiner(func=lambda: self.prompts_db["VLM_PROMPT_1"])
        
        # ç”Ÿæˆåˆç¨¿ (ä½¿ç”¨é€šç”¨ PromptedVQAGenerator)
        self.gen_init_caption = PromptedVQAGenerator(
            serving=self.serving,
            system_prompt="You are a helpful assistant."
        )

        # --- Step B: Refine Golden Sentences ---
        # åˆ†å¥
        self.refine_split = FunctionalRefiner(func=split_sentences)
        
        # è§†è§‰è‡ªæ£€ (ä¿ç•™ Yes çš„å¥å­)
        self.refine_golden = VisualGroundingRefiner(
            serving=self.serving,
            prompt_template="Given the image, is the description '{text}' directly supported by visual evidence? Answer strictly yes or no."
        )

        # --- Step C: Generate Questions ---
        # åˆ—è¡¨è½¬å­—ç¬¦ä¸²
        self.refine_join = FunctionalRefiner(func=join_list)
        
        # æ–‡æœ¬ç”Ÿæˆé—®é¢˜ (Text-to-Text)
        tpl_q = NamedPlaceholderPromptTemplate(
            template=self.prompts_db["LLM_PROMPT_1"], 
            join_list_with="\n"
        )
        self.gen_questions_text = PromptTemplatedQAGenerator(
            serving=self.serving,
            prompt_template=tpl_q
        )
        
        # è§£æé—®é¢˜æ–‡æœ¬ä¸ºåˆ—è¡¨
        self.refine_parse_qs = FunctionalRefiner(func=parse_questions_logic)

        # --- Step D: Generate Answers ---
        # æ‰¹é‡å›ç­” (One Image -> Many Qs)
        self.gen_answers = BatchVQAGenerator(serving=self.serving)
        
        # å›ç­”è¿‡æ»¤
        self.refine_answers = VisualGroundingRefiner(
            serving=self.serving,
            prompt_template="Given the image, is the statement '{text}' grounded in the image and not generic? Answer strictly yes or no."
        )

        # --- Step E: Integrate Final Caption ---
        # èåˆ (Text-to-Text)
        tpl_final = NamedPlaceholderPromptTemplate(
            template=self.prompts_db["LLM_PROMPT_4"], 
            join_list_with="\n"
        )
        self.gen_final_caption = PromptTemplatedQAGenerator(
            serving=self.serving,
            prompt_template=tpl_final
        )

    def forward(self):
        print(">>> [Pipeline] Step 0: Preparing Prompts...")
        # æ„é€  init_prompt åˆ—
        self.refine_const_prompt.run(
            self.storage.step(), 
            output_key="init_prompt"
        )

        print(">>> [Pipeline] Step 1: Generating Initial Caption...")
        self.gen_init_caption.run(
            self.storage.step(),
            input_prompt_key="init_prompt",
            input_image_key=self.input_image_key,
            output_answer_key="init_caption"
        )

        print(">>> [Pipeline] Step 2: Refining Golden Sentences...")
        self.refine_split.run(
            self.storage.step(), 
            output_key="sentences", 
            text="init_caption"
        )
        self.refine_golden.run(
            self.storage.step(), 
            input_list_key="sentences", 
            input_image_key=self.input_image_key, 
            output_key="golden_sentences"
        )

        print(">>> [Pipeline] Step 3: Generating Details Questions...")
        self.refine_join.run(
            self.storage.step(), 
            output_key="golden_str", 
            data="golden_sentences"
        )
        
        # template: "{sentence}" -> map to col "golden_str"
        self.gen_questions_text.run(
            self.storage.step(), 
            output_answer_key="raw_q_text", 
            sentence="golden_str"
        )
        
        self.refine_parse_qs.run(
            self.storage.step(), 
            output_key="q_list", 
            text="raw_q_text"
        )

        print(">>> [Pipeline] Step 4: Generating & Filtering Answers...")
        self.gen_answers.run(
            self.storage.step(), 
            input_prompts_key="q_list", 
            input_image_key=self.input_image_key, 
            output_key="raw_answers"
        )
        
        self.refine_answers.run(
            self.storage.step(), 
            input_list_key="raw_answers", 
            input_image_key=self.input_image_key, 
            output_key="final_details"
        )

        print(">>> [Pipeline] Step 5: Integrating Final Caption...")
        self.refine_join.run(
            self.storage.step(), 
            output_key="details_str", 
            data="final_details"
        )
        
        # template keys: context, object_info, position_info
        self.gen_final_caption.run(
            self.storage.step(),
            output_answer_key=self.output_key,
            context="golden_str",
            object_info="details_str",
            position_info="details_str" # ç®€åŒ–ï¼šåŒæ—¶ä½œä¸º object å’Œ position ä¿¡æ¯
        )

        print(f">>> [Pipeline] All Done. Result saved to: {self.storage.cache_path}")


if __name__ == "__main__":
    pipe = ImageScaleCaptionPipeline(
        model_path="Qwen/Qwen2.5-VL-3B-Instruct",
        hf_cache_dir="~/.cache/huggingface",
        download_dir="../ckpt/models/Qwen2.5-VL-3B-Instruct",
        device="cuda",
        
        first_entry_file="../example_data/capsbench_images/image_scale_caption_demo.jsonl",
        cache_path="../cache/image_scale_caption",
        file_name_prefix="scalecap",
        
        input_image_key="image",
        output_key="final_caption",
        
        vllm_tensor_parallel_size=1,
        vllm_max_tokens=1024
    )
    
    pipe.forward()

```