---
title: å›¾åƒå®šä½æ€ç»´é“¾ (GCoT) ç”Ÿæˆæµæ°´çº¿
icon: mdi:image-text
createTime: 2026/01/11 20:44:55
permalink: /zh/mm_guide/image_gcot/
---
## 1. æ¦‚è¿°

**å›¾åƒå®šä½æ€ç»´é“¾ (GCoT) ç”Ÿæˆæµæ°´çº¿** æ—¨åœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆ**å¸¦è§†è§‰å®šä½çš„æ€ç»´é“¾ï¼ˆGrounded Chain-of-Thoughtï¼‰**æ•°æ®ã€‚è¯¥æµæ°´çº¿é€šè¿‡å¤šæ­¥æ¨ç†ï¼Œä¸ä»…ç”Ÿæˆå›ç­”é—®é¢˜çš„é€»è¾‘æ­¥éª¤ï¼Œè¿˜å°†æ¨ç†è¿‡ç¨‹ä¸­æåˆ°çš„å…³é”®ç‰©ä½“åœ¨å›¾åƒä¸­è¿›è¡Œç©ºé—´å®šä½ï¼ˆBounding Boxï¼‰ï¼Œä»è€Œæ˜¾è‘—æå‡å¤šæ¨¡æ€æ•°æ®çš„å¯è§£é‡Šæ€§å’Œç²¾ç¡®åº¦ã€‚

ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæœ¬æµæ°´çº¿é‡‡ç”¨ **å•ä¸€ VLMï¼ˆå¦‚ Qwen2.5-VLï¼‰** åŒæ—¶å®Œæˆâ€œæ¨ç†â€å’Œâ€œå®šä½â€ä»»åŠ¡ï¼Œæµç¨‹æ›´åŠ ç²¾ç®€é«˜æ•ˆã€‚

æˆ‘ä»¬æ”¯æŒä»¥ä¸‹åº”ç”¨åœºæ™¯ï¼š

* **å¢å¼ºå‹å¤šæ¨¡æ€æ•°æ®æ„å»º**ï¼šä¸º VQA æ•°æ®é›†å¢åŠ è§£é‡Šæ€§å’Œå®šä½æ ‡æ³¨ã€‚
* **å¤æ‚åœºæ™¯ç†è§£**ï¼šç”ŸæˆåŒ…å«ç‰©ä½“åæ ‡çš„è¯¦ç»†æ¨ç†æ­¥éª¤ã€‚
* **æ¨¡å‹æ¨ç†èƒ½åŠ›è®­ç»ƒ**ï¼šæ„å»ºæ•°æ®ä»¥è®­ç»ƒæ¨¡å‹â€œè¨€ä¹‹æœ‰ç‰©â€ï¼Œå‡å°‘å¹»è§‰ã€‚

æµæ°´çº¿çš„ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š

1. **CoT ç”Ÿæˆ**ï¼šæ¨¡å‹ç”Ÿæˆåˆ†æ­¥æ¨ç†æ–‡æœ¬ï¼Œå¹¶æå–å…³é”®åè¯ã€‚
2. **å…³é”®è¯è§£æ**ï¼šä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æ¸…æ´—å¹¶æå–å¾…å®šä½çš„å…³é”®è¯ã€‚
3. **è§†è§‰å®šä½ (Grounding)**ï¼šæ¨¡å‹é’ˆå¯¹æå–çš„å…³é”®è¯ç”Ÿæˆè¾¹ç•Œæ¡† (BBox)ã€‚
4. **ä¿¡æ¯æ³¨å…¥**ï¼šå°† BBox åæ ‡å›å¡«è‡³æ¨ç†æ–‡æœ¬ä¸­ï¼Œå½¢æˆæœ€ç»ˆçš„ GCoTã€‚

---

## 2. å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ–°çš„ DataFlow å·¥ä½œæ–‡ä»¶å¤¹
```bash
mkdir run_dataflow
cd run_dataflow
```

### ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ– DataFlow-MM
```bash
dataflowmm init
```
è¿™æ—¶ä½ ä¼šçœ‹åˆ°ï¼š
```bash
gpu_pipelines/image_gcot_pipeline.py
```

### ç¬¬ä¸‰æ­¥ï¼šä¸‹è½½ç¤ºä¾‹æ•°æ®
```bash
huggingface-cli download --repo-type dataset OpenDCAI/dataflow-demo-image --local-dir ./example_data
```

### ç¬¬å››æ­¥ï¼šé…ç½®å‚æ•°

```bash
if __name__ == "__main__":
    pipe = ImageGCoTPipeline(
        model_path="Qwen/Qwen2.5-VL-3B-Instruct",
        first_entry_file="../example_data/capsbench_images/image_gcot_demo.jsonl",
        hf_cache_dir="~/.cache/huggingface",
        download_dir="../ckpt/models/Qwen2.5-VL-3B-Instruct",
    )
    pipe.forward()

```
> **âš ï¸ æ¨¡å‹è·¯å¾„é…ç½®çš„é‡è¦æç¤ºï¼ˆä»¥ `Qwen2.5-VL-3B-Instruct` ä¸ºä¾‹ï¼‰ï¼š**
> 
> * **å¦‚æœæ‚¨å·²ç»ä¸‹è½½å¥½äº†æ¨¡å‹æ–‡ä»¶**ï¼šè¯·å°† `model_path` ä¿®æ”¹ä¸ºæ‚¨çš„æœ¬åœ°æ¨¡å‹è·¯å¾„ã€‚**åŠ¡å¿…ä¿è¯**æ¨¡å‹å­˜æ”¾çš„æœ€ç»ˆæ–‡ä»¶å¤¹åç§°ç²¾ç¡®ä¸º `Qwen2.5-VL-3B-Instruct`ï¼Œå¦åˆ™åº•å±‚è§£ææ—¶å°†æ— æ³•æ­£ç¡®åŒ¹é…å’Œè¯†åˆ«è¯¥æ¨¡å‹ã€‚
> * **å¦‚æœæ‚¨è¿˜æœªä¸‹è½½æ¨¡å‹ï¼ˆéœ€è¦è‡ªåŠ¨ä¸‹è½½ï¼‰**ï¼šè¯·ä¸€å®šè¦æŒ‡å®š `download_dir` å‚æ•°ï¼Œå¹¶ä¸”è¯¥ç›®å½•è·¯å¾„**å¿…é¡»ä»¥** `Qwen2.5-VL-3B-Instruct` **ç»“å°¾**ï¼ˆæ­£å¦‚é»˜è®¤å‚æ•°æ‰€ç¤ºï¼‰ï¼Œå¦åˆ™ä¸‹è½½å®ŒæˆååŒæ ·ä¼šå¯¼è‡´æ¡†æ¶æ— æ³•è¯†åˆ«æ¨¡å‹ã€‚

### ç¬¬äº”æ­¥ï¼šä¸€é”®è¿è¡Œ

```bash
cd gpu_pipelines
python image_gcot_pipeline.py
```
> **ğŸ› ï¸ å¸¸è§é—®é¢˜æ’æŸ¥ (Troubleshooting)**
> 
> **é—®é¢˜ 1ï¼š** å¦‚æœé‡åˆ°ç±»ä¼¼å¦‚ä¸‹çš„åŠ¨æ€é“¾æ¥åº“å†²çªæŠ¥é”™ï¼š
> `ImportError: .../miniconda3/envs/Dataflow-MM/lib/python3.12/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12`
> 
> **è§£å†³æ–¹æ³•ï¼š** è¿™é€šå¸¸æ˜¯ç¯å¢ƒå˜é‡å¹²æ‰°å¯¼è‡´çš„ã€‚è¯·åœ¨è¿è¡Œå‘½ä»¤å‰æ¸…ç©º `LD_LIBRARY_PATH`ï¼š
> ```bash
> LD_LIBRARY_PATH="" python image_gcot_pipeline.py
> ```
> 
> **é—®é¢˜ 2ï¼š** å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ **Qwen ç³»åˆ—æ¨¡å‹**ï¼Œå¹¶ä¸”é‡åˆ°ä»¥ä¸‹æŠ¥é”™ï¼š
> `KeyError: "Missing required keys in rope_scaling for 'rope_type'='None': {'rope_type'}"`
> 
> **è§£å†³æ–¹æ³•ï¼š** æ‰“å¼€æ¨¡å‹æ–‡ä»¶å¤¹ä¸‹çš„ `config.json` æ–‡ä»¶ï¼Œæ‰¾åˆ° `rope_scaling` é…ç½®å—ï¼Œå°† `"type"` å­—æ®µä¿®æ”¹ä¸º `"rope_type"` å³å¯ã€‚
> 
> **ä¿®æ”¹å‰ï¼š**
> ```json
> "rope_scaling": {
>   "type": "mrope",
>   "mrope_section": [
>     16,
>     24,
>     24
>   ]
> }
> ```
> 
> **ä¿®æ”¹åï¼š**
> ```json
> "rope_scaling": {
>   "rope_type": "mrope",
>   "mrope_section": [
>     16,
>     24,
>     24
>   ]
> }
> ```

---

## 3. æ•°æ®æµä¸æµæ°´çº¿é€»è¾‘

### 1. **è¾“å…¥æ•°æ®**

è¯¥æµç¨‹çš„è¾“å…¥æ•°æ®é€šå¸¸æ˜¯æ ‡å‡†çš„ VQA æ•°æ®ï¼š

* **image**ï¼šå›¾åƒæ–‡ä»¶è·¯å¾„ã€‚
* **question**ï¼šå…³äºå›¾åƒçš„é—®é¢˜ã€‚
* **answer**ï¼šé—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆï¼ˆç”¨äºè¾…åŠ©ç”Ÿæˆ CoTï¼‰ã€‚

**è¾“å…¥æ•°æ®ç¤ºä¾‹**ï¼š

```json
{
    "image":"../example_data/capsbench_images/0.png",
    "question":"Who is the lead actor in the movie \"Nightmare Alley\"?", 
    "answer": "Bradley Cooper."
}

```

### 2. **æ ¸å¿ƒç®—å­é€»è¾‘**

æœ¬æµæ°´çº¿é€šè¿‡ç»„åˆå¤šä¸ªç»†ç²’åº¦ç®—å­æ¥å®ç°å¤æ‚çš„ GCoT ç”Ÿæˆé€»è¾‘ï¼š

#### A. **CoT ç”Ÿæˆ (PromptTemplatedVQAGenerator)**

åˆ©ç”¨é¢„è®¾çš„ `GCOT_PROMPT_TEMPLATE`ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆâ€œæ­¥éª¤åŒ–æ¨ç†â€å’Œâ€œå…³é”®è¯åˆ—è¡¨â€ã€‚

* **Prompt ç­–ç•¥**ï¼šè¦æ±‚æ¨¡å‹æŒ‰ `Step 1: ...`, `Step 2: ...`, `Keywords: ...` æ ¼å¼è¾“å‡ºã€‚
* **è¾“å‡º**ï¼šåŒ…å«æ¨ç†æ–‡æœ¬å’Œå…³é”®è¯çš„åŸå§‹å­—ç¬¦ä¸²ã€‚

#### B. **æ–‡æœ¬æ¸…æ´—ä¸æå– (FunctionalRefiner)**

ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°å¯¹ä¸Šä¸€æ­¥çš„è¾“å‡ºè¿›è¡Œè§£æï¼š

* `extract_clean_cot_logic`ï¼šå‰¥ç¦»å…³é”®è¯éƒ¨åˆ†ï¼Œä¿ç•™çº¯å‡€çš„ CoT æ–‡æœ¬ã€‚
* `extract_keywords_logic`ï¼šè§£æ `Keywords:` åçš„å†…å®¹ï¼Œç”Ÿæˆ Python Listã€‚

#### C. **è§†è§‰å®šä½ (VLMBBoxGenerator)**

é’ˆå¯¹æå–å‡ºçš„æ¯ä¸€ä¸ªå…³é”®è¯ï¼Œè°ƒç”¨ VLM çš„å®šä½èƒ½åŠ›ç”Ÿæˆè¾¹ç•Œæ¡†ã€‚

* **è¾“å…¥**ï¼šå›¾åƒ + å…³é”®è¯åˆ—è¡¨ã€‚
* **è¾“å‡º**ï¼šå…³é”®è¯åˆ°è¾¹ç•Œæ¡†åæ ‡çš„æ˜ å°„å­—å…¸ (Map)ã€‚

#### D. **åæ ‡æ³¨å…¥ (FunctionalRefiner)**

ä½¿ç”¨ `inject_bboxes_logic` å‡½æ•°ï¼Œå°†ç”Ÿæˆçš„ BBox åæ ‡æ™ºèƒ½æ’å…¥å›åŸå§‹ CoT æ–‡æœ¬ä¸­å¯¹åº”çš„å•è¯ä¹‹åã€‚

### 3. **è¾“å‡ºæ•°æ®**

æœ€ç»ˆï¼Œæµæ°´çº¿ç”Ÿæˆçš„è¾“å‡ºæ•°æ®å°†åŒ…å«ä»¥ä¸‹å…³é”®å­—æ®µï¼š

* **raw_cot_output**ï¼šæ¨¡å‹åŸå§‹ç”Ÿæˆçš„æ–‡æœ¬ã€‚
* **cleaned_cot**ï¼šæ¸…æ´—åçš„çº¯æ¨ç†æ–‡æœ¬ã€‚
* **bbox_mapping**ï¼šå…³é”®è¯ä¸å…¶åæ ‡çš„æ˜ å°„ã€‚
* **gcot**ï¼šæœ€ç»ˆç»“æœï¼ŒåŒ…å«åæ ‡ä¿¡æ¯çš„æ¨ç†é“¾ã€‚

**è¾“å‡ºæ•°æ®ç¤ºä¾‹ (gcot å­—æ®µ)**ï¼š

```text
Step 1: Analyze the text visible in the image, which includes a list of actors beneath the title of the movie \"Nightmare Alley.\"\n\nStep 2: Identify the names listed. The first name listed is \"Bradley Cooper,\" indicating he is prominent in the film.\n\nStep 3: Recognize that the image is a promotional poster for \"Nightmare Alley,\" suggesting the individuals mentioned are likely key cast members.\n\nStep 4: Confirm that Bradley Cooper is identified as the lead actor based on his position at the top of the cast list.\n\nAnswer: Bradley Cooper.  \nKeywords: Nightmare Alley, cast list, poster.","cleaned_cot":"Step 1: Analyze the text visible in the image, which includes a list of actors beneath the title of the movie \"Nightmare Alley.\"\n\nStep 2: Identify the names listed. The first name listed is \"Bradley Cooper,\" indicating he is prominent in the film.\n\nStep 3: Recognize that the image is a promotional poster for \"Nightmare Alley,\" suggesting the individuals mentioned are likely key cast members.\n\nStep 4: Confirm that Bradley Cooper is identified as the lead actor based on his position at the top of the cast list.\n\nAnswer: Bradley Cooper.","extracted_keywords":["Nightmare Alley","cast list","poster"],"bbox_mapping":{},"gcot":"Step 1: Analyze the text visible in the image, which includes a list of actors beneath the title of the movie \"Nightmare Alley.\"\n\nStep 2: Identify the names listed. The first name listed is \"Bradley Cooper,\" indicating he is prominent in the film.\n\nStep 3: Recognize that the image is a promotional poster for \"Nightmare Alley,\" suggesting the individuals mentioned are likely key cast members.\n\nStep 4: Confirm that Bradley Cooper is identified as the lead actor based on his position at the top of the cast list.\n\nAnswer: Bradley Cooper.

```

---

## 4. æµæ°´çº¿ç¤ºä¾‹

ä»¥ä¸‹æ˜¯å®Œæ•´çš„ `ImageGCoTPipeline` ä»£ç å®ç°ã€‚

```python
import re
from typing import List, Dict, Any
import argparse
import gc
import torch
from dataflow.utils.storage import FileStorage
from dataflow.serving.local_model_vlm_serving import LocalModelVLMServing_vllm

from dataflow.operators.core_vision import PromptTemplatedVQAGenerator, VLMBBoxGenerator
from dataflow.operators.core_text import FunctionalRefiner
from dataflow.prompts.prompt_template import NamedPlaceholderPromptTemplate

GCOT_PROMPT_TEMPLATE = (
    "Question: {question}\n"
    "Answer: {answer}\n\n"
    "Task: Provide a detailed step-by-step reasoning (Chain-of-Thought) that explains "
    "how to arrive at this answer based on the image.\n"
    "Then, extract key nouns and objects mentioned in your reasoning that are "
    "visible in the image and can be spatially located.\n\n"
    "Format:\n"
    "Step 1: ...\n"
    "Step 2: ...\n"
    "Answer: {answer}\n"
    "Keywords: object1, object2\n"
)

DEFAULT_BBOX_PROMPT = 'Detect "{keyword}".'

def _parse_base(text: str) -> Dict[str, Any]:
    """åŸºç¡€è§£æé€»è¾‘ï¼ˆå†…éƒ¨å¤ç”¨ï¼‰"""
    if not text: return {"cot": "", "keywords": []}
    lines = text.split('\n')
    cot_lines = []
    keywords = []
    for line in lines:
        if line.strip().lower().startswith('keywords:'):
            keyword_str = line.split(':', 1)[-1].strip()
            raw_kws = [kw.strip().strip('.,;:!?"\'') for kw in keyword_str.replace(';', ',').split(',')]
            keywords = [k for k in raw_kws if k]
        else:
            cot_lines.append(line)
    return {"cot": '\n'.join(cot_lines).strip(), "keywords": keywords}

def extract_clean_cot_logic(text: str) -> str:
    """[For FunctionalRefiner] ä»…è¿”å›æ¸…æ´—åçš„ CoT æ–‡æœ¬"""
    return _parse_base(text)["cot"]

def extract_keywords_logic(text: str) -> List[str]:
    """[For FunctionalRefiner] æå–å¹¶åˆå¹¶å…³é”®è¯"""
    parsed = _parse_base(text)
    kws = parsed["keywords"]
    cot = parsed["cot"]
    
    if not kws or len(kws) <= 1:
        return kws
    
    # ç®€å•çš„ç›¸é‚»åˆå¹¶é€»è¾‘
    cot_lower = cot.lower()
    merged = []
    skip_indices = set()
    for i in range(len(kws)):
        if i in skip_indices: continue
        best_match = kws[i]
        best_indices = [i]
        # å°è¯•å‘ååˆå¹¶ 3 ä¸ªè¯
        for j in range(i + 1, min(i + 4, len(kws))):
            if j in skip_indices: break
            combined = ' '.join(kws[i:j+1])
            if combined.lower() in cot_lower:
                best_match = combined
                best_indices = list(range(i, j+1))
            else: break
        merged.append(best_match)
        skip_indices.update(best_indices)
    return merged

def inject_bboxes_logic(cot_text: str, bbox_map: Dict[str, List[str]]) -> str:
    """[For FunctionalRefiner] å°† BBox æ³¨å…¥å› CoT"""
    if not cot_text or not bbox_map: return cot_text
    # ä¼˜å…ˆåŒ¹é…é•¿è¯
    sorted_keywords = sorted(bbox_map.keys(), key=lambda x: len(x), reverse=True)
    result_text = cot_text
    replaced = set()
    
    for keyword in sorted_keywords:
        if keyword in replaced: continue
        # ç®€å•ç­–ç•¥ï¼šåªåœ¨ 'Answer:' ä¹‹å‰æ³¨å…¥ï¼Œé˜²æ­¢ç ´åç­”æ¡ˆåŒº
        answer_pos = result_text.find('Answer:')
        search_limit = answer_pos if answer_pos != -1 else len(result_text)
        
        pos = result_text.lower().find(keyword.lower(), 0, search_limit)
        if pos == -1: continue
        
        boxes = bbox_map[keyword] # List[str]
        box_str = "".join(boxes)
        replacement = f"{keyword} {box_str}"
        
        result_text = result_text[:pos] + replacement + result_text[pos + len(keyword):]
        replaced.add(keyword)
    return result_text

class ImageGCoTPipeline:
    def __init__(
        self,
        model_path: str,
        *,
        hf_cache_dir: str | None = None,
        download_dir: str = "./ckpt/models",
        first_entry_file: str,
        cache_path: str = "../cache/cache_gcot",
        file_name_prefix: str = "gcot",
        # Keys
        question_key: str = "question",
        answer_key: str = "answer",
        image_key: str = "image",
        output_key: str = "gcot",
        # Config
        vllm_max_tokens: int = 512
    ):
        self.storage = FileStorage(
            first_entry_file_name=first_entry_file,
            cache_path=cache_path,
            file_name_prefix=file_name_prefix,
            cache_type="jsonl"
        )
        
        # [å•ä¸€æ¨¡å‹ Serving]
        self.vlm_serving = LocalModelVLMServing_vllm(
            hf_model_name_or_path=model_path,
            hf_cache_dir=hf_cache_dir,
            hf_local_dir=download_dir,
            vllm_tensor_parallel_size=1,
            vllm_temperature=0.7,
            vllm_max_tokens=vllm_max_tokens
        )
        
        self.keys = {
            "q": question_key,
            "a": answer_key,
            "img": image_key,
            "raw_cot": "raw_cot_output",
            "clean_cot": "cleaned_cot",
            "keywords": "extracted_keywords",
            "bbox_map": "bbox_mapping",
            "final": output_key
        }

        # ================== Operators ==================
        
        # 1. Generate CoT (é€šç”¨ Generator)
        self.op_gen_cot = PromptTemplatedVQAGenerator(
            serving=self.vlm_serving,
            system_prompt="You are a helpful assistant.",
            prompt_template=NamedPlaceholderPromptTemplate(template=GCOT_PROMPT_TEMPLATE)
        )
        
        # 2. Extract Clean CoT (é€šç”¨ Refiner + Helper)
        self.op_extract_cot = FunctionalRefiner(func=extract_clean_cot_logic)
        
        # 3. Extract Keywords (é€šç”¨ Refiner + Helper)
        self.op_extract_kws = FunctionalRefiner(func=extract_keywords_logic)

        # 4. Generate BBox (ä¸“ç”¨ Generator, å› ä¸ºæ¶‰åŠè¡Œå†… Batch)
        self.op_bbox_gen = VLMBBoxGenerator(
            serving=self.vlm_serving,
            prompt_template=DEFAULT_BBOX_PROMPT
        )
        
        # 5. Inject GCoT (é€šç”¨ Refiner + Helper)
        self.op_inject = FunctionalRefiner(func=inject_bboxes_logic)

    def forward(self):
        print(">>> [Pipeline] Step 1: Generating CoT...")
        self.op_gen_cot.run(
            self.storage.step(),
            input_image_key=self.keys["img"],
            output_answer_key=self.keys["raw_cot"],
            question=self.keys["q"], # Template mapping
            answer=self.keys["a"]
        )
        
        print(">>> [Pipeline] Step 2: Parsing Outputs...")
        self.op_extract_cot.run(
            self.storage.step(),
            output_key=self.keys["clean_cot"],
            text=self.keys["raw_cot"] # Param mapping
        )
        self.op_extract_kws.run(
            self.storage.step(),
            output_key=self.keys["keywords"],
            text=self.keys["raw_cot"]
        )
        
        print(">>> [Pipeline] Step 3: Generating BBoxes (Grounding)...")
        self.op_bbox_gen.run(
            self.storage.step(),
            input_image_key=self.keys["img"],
            input_kws_key=self.keys["keywords"],
            output_key=self.keys["bbox_map"]
        )
        
        print(">>> [Pipeline] Step 4: Injecting GCoT...")
        self.op_inject.run(
            self.storage.step(),
            output_key=self.keys["final"],
            cot_text=self.keys["clean_cot"],
            bbox_map=self.keys["bbox_map"]
        )
        
        print(f">>> [Pipeline] Done. Final GCoT saved to: {self.keys['final']}")


if __name__ == "__main__":
    pipe = ImageGCoTPipeline(
        model_path="Qwen/Qwen2.5-VL-3B-Instruct",
        first_entry_file="../example_data/capsbench_images/image_gcot_demo.jsonl",
        hf_cache_dir="~/.cache/huggingface",
        download_dir="../ckpt/models/Qwen2.5-VL-3B-Instruct",
    )
    pipe.forward()
```
